{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a64b3a4",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=TSyjBZKL5XA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed3b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/ipython/ipyparallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b025045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install faker\n",
    "# If needed: Install findspark to make available spark from jupyter-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2b6b513-73b6-4415-b6d9-704561775908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b457e2b5-a2e0-4235-870f-02e3f7b051c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc77a985-2b94-4b4a-af9e-e912038217da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9ea5eb-ecdb-42aa-8259-d930cbb66b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaaccf6",
   "metadata": {},
   "source": [
    "catalog_nm = \"iceberg_catalog\"\n",
    "warehouse_path = \"warehouse_loc\"\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"iceberg_with_filesystem\")\n",
    "    #pachages\n",
    "    .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2\")  # Corrected coordinate\n",
    "    #SQL Extensions\n",
    "    .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions:IcebergSparkSessionExtensions')\n",
    "    #Configuring catalog\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}','org.apache.iceberg.spark.SparkCatalog')\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}.type', 'hadoop')\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}.warehouse', warehouse_path)\n",
    ")\n",
    "\n",
    "#Start SparkSession\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d463a2e-047d-4883-bc63-adccb705b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logger for your application\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # You can set to DEBUG, INFO, WARNING, ERROR, or CRITICAL\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Log format\n",
    "    handlers=[logging.StreamHandler()]  # Output logs to the console\n",
    ")\n",
    "\n",
    "# Create a logger object for your application\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc3dba",
   "metadata": {},
   "source": [
    "## Improved conf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2db2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 21:50:57 WARN Utils: Your hostname, MacBook-Pro-de-Victor.local resolves to a loopback address: 127.0.0.1; using 192.168.1.141 instead (on interface en0)\n",
      "24/10/22 21:50:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/victorgalan/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/victorgalan/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/victorgalan/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.4_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fd851c7c-0353-4a73-8606-bfef895edd10;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.5.2 in central\n",
      ":: resolution report :: resolve 190ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.4_2.12;1.5.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fd851c7c-0353-4a73-8606-bfef895edd10\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/10ms)\n",
      "24/10/22 21:50:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-10-22 21:51:03,031 - __main__ - INFO - Spark session is up and running!\n"
     ]
    }
   ],
   "source": [
    "catalog_nm = \"iceberg_catalog\" \n",
    "warehouse_path = \"warehouse_loc\"\n",
    "\n",
    "# Configure Spark session for Iceberg\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"iceberg_with_filesystem\")\n",
    "    .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.2\")\n",
    "    .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}.type', 'hadoop')\n",
    "    .set(f'spark.sql.catalog.{catalog_nm}.warehouse', warehouse_path)\n",
    ")\n",
    "\n",
    "# Start SparkSession\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "logger.info(\"Spark session is up and running!\")\n",
    "\n",
    "# Adjust log level to suppress warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2002ad-bf6e-43f6-96dc-29af8a6a8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a query\n",
    "database_nm = \"my_database\"\n",
    "table_nm = \"my_part_table\"\n",
    "\n",
    "my_table = f\"{catalog_nm}.{database_nm}.{table_nm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebadafaa-a09a-46d0-a98d-70d480ee2209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {my_table} (\n",
    "            name STRING,\n",
    "            surname STRING,\n",
    "            country STRING\n",
    "            ) \n",
    "            using iceberg\n",
    "            PARTITIONED BY (birthday DATE);\n",
    "            \"\"\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aab0a1a-689c-4139-bc8b-58d1029a674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d24872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data(num_records_to_create):\n",
    "    num_inserts = num_records_to_create\n",
    "    date_range = (-10, -1)  # Años desde el presente\n",
    "    \n",
    "    # Lista de países (puedes usar faker.country() si prefieres)\n",
    "    #countries = ['United States', 'Canada', 'United Kingdom', ...]\n",
    "    \n",
    "    # Generar datos aleatorios\n",
    "    fake = Faker()\n",
    "    data = []\n",
    "    for _ in range(num_inserts):\n",
    "        data.append({\n",
    "            'name': fake.name(),\n",
    "            'surname': fake.last_name(),\n",
    "            'birthday': datetime.strptime(fake.date_between(start_date=f'{date_range[0]}y', end_date=f'{date_range[1]}y').strftime('%Y-%m-%d'), '%Y-%m-%d'),\n",
    "            #'country': str(random.choice(countries))  # O str(fake.random_element(elements=countries))\n",
    "            'country': fake.country() \n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37918b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Shelly Guzman',\n",
       "  'surname': 'Todd',\n",
       "  'birthday': datetime.datetime(2019, 10, 16, 0, 0),\n",
       "  'country': 'Samoa'},\n",
       " {'name': 'Mr. Jeremy Riley',\n",
       "  'surname': 'Howard',\n",
       "  'birthday': datetime.datetime(2020, 4, 29, 0, 0),\n",
       "  'country': 'Tajikistan'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fake_data(2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ce6fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_fake_data_into_table(num_records_to_create):\n",
    "    data = fake_data(num_records_to_create)\n",
    "    print(data)\n",
    "    # Create DataFrame from valid data\n",
    "    df = spark.createDataFrame(data)\n",
    "    df = df.withColumn(\"birthday\", df[\"birthday\"].cast(\"date\"))\n",
    "    \n",
    "    df.createOrReplaceTempView(\"fake_data\")\n",
    "    \n",
    "    #df.printSchema()\n",
    "    #df.show()\n",
    "    \n",
    "    logger.info(f\"Number of records to be inserted: {df.count()}\")\n",
    "    query_view = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM fake_data\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "    spark.sql(query_view).show()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {my_table}\n",
    "    SELECT name, \n",
    "            surname, \n",
    "            country, \n",
    "            to_date(birthday, 'yyyy-MM-dd') AS birthday\n",
    "    FROM fake_data;\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(query).show()\n",
    "    logger.info(f\"Records inserted successfully!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0507007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Matthew Cook', 'surname': 'Green', 'birthday': datetime.datetime(2020, 2, 20, 0, 0), 'country': 'Cuba'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 22:25:41,278 - __main__ - INFO - Number of records to be inserted: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------+-------+\n",
      "|  birthday|country|        name|surname|\n",
      "+----------+-------+------------+-------+\n",
      "|2020-02-20|   Cuba|Matthew Cook|  Green|\n",
      "+----------+-------+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 22:25:42,232 - __main__ - INFO - Records inserted successfully!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_fake_data_into_table(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e454ca-45f3-4d26-abc4-ebc242d11c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Insert into with dataframe api\n",
    "\n",
    "#df = spark.createDataFrame([Row(name=\"Sr. victor\",surname=\"galan\")])\n",
    "#df.writeTo(f\"{catalog_nm}.{table_nm}\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "782277bc-1a70-46fe-bfe6-23fe64c1c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_my_table():\n",
    "    #Query the table\n",
    "    spark.sql(\n",
    "        f\"\"\"SELECT * \n",
    "        FROM {my_table};\n",
    "        \"\"\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688411bb-795c-45bf-b350-d22dc2fd5b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+----------+----------+\n",
      "|            name|surname|   country|  birthday|\n",
      "+----------------+-------+----------+----------+\n",
      "|   Shelly Guzman|   Todd|     Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|Tajikistan|2020-04-29|\n",
      "+----------------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_my_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d5d1d75-13e5-4941-af75-b94e3a98726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new database in the catalog\n",
    "try:\n",
    "    spark.sql(f\"CREATE DATABASE {catalog_nm}.{table_nm}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6b9caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3f21f9-c620-4c04-a167-31f8b0e1baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4e012-af4d-4934-ba90-3e64ff1bd5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4138579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_extended():\n",
    "    describe_extended_pyspark_df = spark.sql(f\"DESCRIBE EXTENDED {my_table}\")\n",
    "    #describe_extended_pyspark_df.show(truncate=False)\n",
    "    \n",
    "    # Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "    describe_extended_df = describe_extended_pyspark_df.select(\"*\").toPandas()\n",
    "    return describe_extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb4b614c-0648-4f59-989d-91ff92b19293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>string</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surname</td>\n",
       "      <td>string</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>country</td>\n",
       "      <td>string</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>birthday</td>\n",
       "      <td>date</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td># Metadata Columns</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>_spec_id</td>\n",
       "      <td>int</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_partition</td>\n",
       "      <td>struct&lt;birthday:date&gt;</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_file</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>_pos</td>\n",
       "      <td>bigint</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_deleted</td>\n",
       "      <td>boolean</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td># Detailed Table Information</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Name</td>\n",
       "      <td>iceberg_catalog.my_database.my_part_table</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Type</td>\n",
       "      <td>MANAGED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Location</td>\n",
       "      <td>warehouse_loc/my_database/my_part_table</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Provider</td>\n",
       "      <td>iceberg</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Owner</td>\n",
       "      <td>victorgalan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Table Properties</td>\n",
       "      <td>[current-snapshot-id=5008345514129221719,forma...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        col_name  \\\n",
       "0                           name   \n",
       "1                        surname   \n",
       "2                        country   \n",
       "3                       birthday   \n",
       "4                                  \n",
       "5             # Metadata Columns   \n",
       "6                       _spec_id   \n",
       "7                     _partition   \n",
       "8                          _file   \n",
       "9                           _pos   \n",
       "10                      _deleted   \n",
       "11                                 \n",
       "12  # Detailed Table Information   \n",
       "13                          Name   \n",
       "14                          Type   \n",
       "15                      Location   \n",
       "16                      Provider   \n",
       "17                         Owner   \n",
       "18              Table Properties   \n",
       "\n",
       "                                            data_type comment  \n",
       "0                                              string    None  \n",
       "1                                              string    None  \n",
       "2                                              string    None  \n",
       "3                                                date    None  \n",
       "4                                                              \n",
       "5                                                              \n",
       "6                                                 int          \n",
       "7                               struct<birthday:date>          \n",
       "8                                              string          \n",
       "9                                              bigint          \n",
       "10                                            boolean          \n",
       "11                                                             \n",
       "12                                                             \n",
       "13          iceberg_catalog.my_database.my_part_table          \n",
       "14                                            MANAGED          \n",
       "15            warehouse_loc/my_database/my_part_table          \n",
       "16                                            iceberg          \n",
       "17                                        victorgalan          \n",
       "18  [current-snapshot-id=5008345514129221719,forma...          "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_extended_df = describe_extended()\n",
    "describe_extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88aeacd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+----------+----------+\n",
      "|            name|surname|   country|  birthday|\n",
      "+----------------+-------+----------+----------+\n",
      "|   Shelly Guzman|   Todd|     Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|Tajikistan|2020-04-29|\n",
      "+----------------+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_my_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc581589-fc9e-4304-bb74-d83869fa5c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      "\n",
      "+-----+--------+-------+----------+\n",
      "|name |surname |country|birthday  |\n",
      "+-----+--------+-------+----------+\n",
      "|jose |garcia  |mexico |2020-03-03|\n",
      "|pedro|gonzalez|brazil |2022-03-03|\n",
      "+-----+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"jose\", \"garcia\", \"mexico\", \"2020-03-03\"),\n",
    "    (\"pedro\", \"gonzalez\", \"brazil\",\"2022-03-03\")]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"surname\",StringType(),True),\n",
    "    StructField(\"country\",StringType(),True),\n",
    "    StructField(\"birthday\", StringType(), True),\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df = df.withColumn(\"birthday\", F.col(\"birthday\").cast(DateType()))\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4ace725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_partition_column(my_table, col_str):\n",
    "    query_evo = f'''\n",
    "    ALTER TABLE {my_table}\n",
    "    ADD PARTITION FIELD {col_str}\n",
    "    '''\n",
    "    spark.sql(query_evo)\n",
    "    logging.info(f\"Patition added {col_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20f28949-b387-4e64-b43c-6d51f7f4f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 23:06:57,034 - root - INFO - Patition added birthday\n"
     ]
    }
   ],
   "source": [
    "add_one_partition_column(my_table, \"birthday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0cca5b1b-5976-4b8c-887c-34c6b1129456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+\n",
      "|            name|surname|      country|  birthday|\n",
      "+----------------+-------+-------------+----------+\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "+----------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_overwrite_df = spark.sql(f\"SELECT * FROM {my_table}\")\n",
    "data_to_overwrite_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b9a1b2cf-925d-4064-906b-4dfdcb7be8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+\n",
      "|            name|surname|      country|  birthday|\n",
      "+----------------+-------+-------------+----------+\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "+----------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_overwrite_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5dae5bca-6e31-4b83-81d5-30a74b760cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    #data_to_overwrite_df.writeTo(my_table).partitionedBy(\"birthday\").overwritePartitions()\n",
    "    data_to_overwrite_df.distinct().writeTo(my_table).overwrite(F.lit(True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c4dbb711-6687-4ff8-a803-98184bb2a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthday: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_overwrite_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "32d1236d-0ae6-48ff-81cb-c6379033d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|col_name               |data_type|comment|\n",
      "+-----------------------+---------+-------+\n",
      "|name                   |string   |null   |\n",
      "|surname                |string   |null   |\n",
      "|country                |string   |null   |\n",
      "|birthday               |date     |null   |\n",
      "|# Partition Information|         |       |\n",
      "|# col_name             |data_type|comment|\n",
      "|birthday               |date     |null   |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the sort order using SQL\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {my_table}\n",
    "    WRITE ORDERED BY (birthday ASC NULLS LAST, name DESC NULLS FIRST)\n",
    "\"\"\")\n",
    "\n",
    "# Verify the change (optional)\n",
    "spark.sql(f\"DESCRIBE {my_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84218f-31f0-4f05-a5f0-64b8380ff884",
   "metadata": {},
   "source": [
    "# VACUUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9e76e4c2-51d7-4ebb-b080-89d7956e3622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4b0fd9dd-d98a-4ebe-8859-fd842ab1933e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vacuum command to clean up old files\n",
    "# Set vacuum properties for the Iceberg table\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {catalog_nm}.{database_nm}.{table_nm}\n",
    "    SET TBLPROPERTIES (\n",
    "        'vacuum_max_snapshot_age_seconds' = {60*5},  -- 1 hour\n",
    "        'vacuum_min_snapshots_to_keep' = '1'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cf5de38f-9693-40fd-b6e2-140d96fb0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-10-22 21:57:...|5917362355917637182|               null|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:23:...|2023042709182525745|5917362355917637182|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:24:...|2928266320472832784|2023042709182525745|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:25:...|3503764429907316250|2928266320472832784|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:25:...| 401056988078697897|3503764429907316250|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:54:...|8819043496371456546| 401056988078697897|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:55:...|6226230512724517284|8819043496371456546|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:55:...|2255682759446792779|6226230512724517284|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:56:...|2129062525253619755|2255682759446792779|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:58:...|6961298048165061712|2129062525253619755|   append|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 22:59:...|5008345514129221719|6961298048165061712|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 23:04:...|6802655846904349513|5008345514129221719|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 23:05:...|5241691938000214284|6802655846904349513|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 23:07:...|1752009770909895026|5241691938000214284|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 23:14:...|8146435919198495740|1752009770909895026|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "|2024-10-22 23:16:...|2230295015391592644|8146435919198495740|overwrite|warehouse_loc/my_...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Snapshots can be queried in a spark session\n",
    "snapshots = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM {my_table}.snapshots;\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62474c-37a1-44f8-89fa-3d1a92b1d7f8",
   "metadata": {},
   "source": [
    "# OPTIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a899fe5b-9153-45fc-997f-3f12a29f8eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'OPTIMIZE'.(line 1, pos 0)\n\n== SQL ==\nOPTIMIZE iceberg_catalog.my_database.my_part_table REWRITE DATA USING BIN_PACK\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optimize the Iceberg table to clean up unused files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mOPTIMIZE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m REWRITE DATA USING BIN_PACK\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'OPTIMIZE'.(line 1, pos 0)\n\n== SQL ==\nOPTIMIZE iceberg_catalog.my_database.my_part_table REWRITE DATA USING BIN_PACK\n^^^\n"
     ]
    }
   ],
   "source": [
    "# Optimize the Iceberg table to clean up unused files\n",
    "spark.sql(f\"\"\"OPTIMIZE {catalog_nm}.{database_nm}.{table_nm} REWRITE DATA USING BIN_PACK\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c2688-8d00-4d1a-bacc-695b0788339e",
   "metadata": {},
   "source": [
    "# compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "20943308-4f4c-4d94-ba9d-32841e6f57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_database'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5b66f1cc-7c52-42b2-ab08-1dba92a38692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------+\n",
      "|  namespace|    tableName|isTemporary|\n",
      "+-----------+-------------+-----------+\n",
      "|my_database|my_part_table|      false|\n",
      "+-----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES IN {catalog_nm}.{database_nm}\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "30c035bd-de1a-481b-874d-244b906c3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         0|                     0|                    0|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CALL {catalog_nm}.system.rewrite_data_files(\n",
    "        table => '{database_nm}.{table_nm}', \n",
    "        strategy => 'sort', \n",
    "        sort_order => 'zorder(birthday, name)',\n",
    "        options => map('min-file-size-bytes', '5242880', 'max-file-size-bytes', '1073741824')\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3874bf0e-cf65-4016-8a34-cf8a324c9606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         0|                     0|                    0|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CALL {catalog_nm}.system.rewrite_data_files(\n",
    "        table => '{database_nm}.{table_nm}', \n",
    "        strategy => 'sort', \n",
    "        sort_order => 'zorder(birthday, name)',\n",
    "        options => map('min-input-files', '5', 'max-concurrent-file-group-rewrites', '10')\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b1a957df-842e-4207-98f3-45e4f4a4c9d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[INVALID_SQL_SYNTAX] Invalid SQL syntax: table valued function cannot specify database name: `iceberg_catalog`.`system`.`files`.(line 1, pos 14)\n\n== SQL ==\nSELECT * FROM iceberg_catalog.system.files('my_database.my_part_table')\n--------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.system.files(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[INVALID_SQL_SYNTAX] Invalid SQL syntax: table valued function cannot specify database name: `iceberg_catalog`.`system`.`files`.(line 1, pos 14)\n\n== SQL ==\nSELECT * FROM iceberg_catalog.system.files('my_database.my_part_table')\n--------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {catalog_nm}.system.files('{database_nm}.{table_nm}')\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e9d72863-4876-456e-ad71-d075135f5520",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Could not build name to arg map: Unknown argument: partition",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[226], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Rewrite data files for a specific partition\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    CALL \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.system.rewrite_data_files(\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        table => \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatabase_nm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmy_table\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m        strategy => \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinpack\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        options => map(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin-file-size-bytes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5242880\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax-file-size-bytes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1073741824\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m        partition => map(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartition_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartition_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    )\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Could not build name to arg map: Unknown argument: partition"
     ]
    }
   ],
   "source": [
    "# Rewrite data files for a specific partition\n",
    "spark.sql(f\"\"\"\n",
    "    CALL {catalog_nm}.system.rewrite_data_files(\n",
    "        table => '{database_nm}.{my_table}', \n",
    "        strategy => 'binpack',\n",
    "        options => map('min-file-size-bytes', '5242880', 'max-file-size-bytes', '1073741824'),\n",
    "        partition => map('partition_column', 'partition_value')\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "31d4aa02-df0e-4d0f-a97b-6b2ecbec1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SHOW CREATE TABLE {my_table}\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d977bc06-0514-4e6f-a506-eb0b24a07991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>_partition</td>\n",
       "      <td>struct&lt;birthday:date&gt;</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col_name              data_type comment\n",
       "10  _partition  struct<birthday:date>        "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_extended_df = describe_extended()\n",
    "describe_extended_df[describe_extended_df[\"col_name\"] == \"_partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54b9d3c9-f62b-42a4-8746-eca2592dacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "378a6e79-f062-4e04-835a-5109053783e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 22:25:02,632 - __main__ - INFO - Number of records to be inserted: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-------+\n",
      "|  birthday|country|         name|surname|\n",
      "+----------+-------+-------------+-------+\n",
      "|2020-09-21|Ireland|Julie Mueller|  Smith|\n",
      "+----------+-------+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 22:25:03,594 - __main__ - INFO - Records inserted successfully!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_fake_data_into_table(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55fa7577-26ca-43e7-9eff-c412f243fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+\n",
      "|            name|surname|      country|  birthday|\n",
      "+----------------+-------+-------------+----------+\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "+----------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_my_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "779a787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_one_partition_column(my_table, col_str):\n",
    "    query_evo = f'''\n",
    "    ALTER TABLE {my_table}\n",
    "    DROP PARTITION FIELD `{col_str}`\n",
    "    '''\n",
    "    spark.sql(query_evo)\n",
    "    logging.info(f\"Patition dropped {col_str}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "980287c0-8432-43aa-a6bb-827310c04df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot find partition field to remove: ref(name=\"birthday\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madd_one_partition_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbirthday\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 6\u001b[0m, in \u001b[0;36madd_one_partition_column\u001b[0;34m(my_table, col_str)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_one_partition_column\u001b[39m(my_table, col_str):\n\u001b[1;32m      2\u001b[0m     query_evo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    ALTER TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_table\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    DROP PARTITION FIELD \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_str\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_evo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatition added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot find partition field to remove: ref(name=\"birthday\")"
     ]
    }
   ],
   "source": [
    "drop_one_partition_column(my_table, \"birthday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c3a1222-c411-4649-869e-b03de3995de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>_partition</td>\n",
       "      <td>struct&lt;birthday:date&gt;</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     col_name              data_type comment\n",
       "7  _partition  struct<birthday:date>        "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_extended_df = describe_extended()\n",
    "describe_extended_df[describe_extended_df[\"col_name\"] == \"_partition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ce0db750-2a28-4c7c-864a-145504298c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|   partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|{2020-02-20}|      0|           4|         1|                         1380|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "|{2019-10-16}|      0|           4|         1|                         1386|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "|{2016-08-24}|      0|           4|         1|                         1400|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "|{2020-04-29}|      0|           4|         1|                         1458|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "|      {null}|      1|          24|         1|                         1587|                           0|                         0|                           0|                         0|2024-10-22 23:05:...|     5241691938000214284|\n",
      "|{2019-08-04}|      0|           4|         1|                         1458|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "|{2020-09-21}|      0|           4|         1|                         1408|                           0|                         0|                           0|                         0|2024-10-22 23:07:...|     1752009770909895026|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_info = spark.sql(f\"SELECT * FROM {my_table}.partitions\")\n",
    "partition_info.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "64f954e9-97e1-4257-9035-acacaa38a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+\n",
      "|            name|surname|      country|  birthday|\n",
      "+----------------+-------+-------------+----------+\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "+----------------+-------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_my_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "28ac1693-0f80-44d3-8539-c8bd90ce9fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+--------+\n",
      "|name|surname|country|birthday|\n",
      "+----+-------+-------+--------+\n",
      "+----+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace `my_table` and `birthday` with your actual table name and partition column\n",
    "null_partition_data = spark.sql(f\"SELECT * FROM {my_table} WHERE birthday IS NULL\")\n",
    "null_partition_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94da85-e006-430c-a717-e823cbdc27f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c2bbb-3ed6-44ec-8535-097d091d0bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0fa47-fe12-4c60-b51b-6cde0618ca8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14d5ed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_evo = f'''\n",
    "ALTER TABLE {my_table}\n",
    "ADD PARTITION FIELD country\n",
    "'''\n",
    "spark.sql(query_evo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c417b635-2d21-46c1-8cce-01e5dbeb6ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+----------+\n",
      "|         name|surname|             country|  birthday|\n",
      "+-------------+-------+--------------------+----------+\n",
      "|    Joseph Ho| Castro|Falkland Islands ...|2019-09-01|\n",
      "|Benjamin Wong| Tucker|             Ireland|2020-09-03|\n",
      "+-------------+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {my_table}\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd4a6c-df48-47db-b255-e517bc709cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "548f3d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.sql.\n: org.apache.iceberg.exceptions.ValidationException: Cannot bind: truncate[4] cannot transform date values from 'birthday'\n\tat org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)\n\tat org.apache.iceberg.expressions.UnboundTransform.bind(UnboundTransform.java:48)\n\tat org.apache.iceberg.expressions.UnboundTransform.bind(UnboundTransform.java:25)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.resolve(BaseUpdatePartitionSpec.java:337)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.addField(BaseUpdatePartitionSpec.java:174)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.addField(BaseUpdatePartitionSpec.java:44)\n\tat org.apache.spark.sql.execution.datasources.v2.AddPartitionFieldExec.run(AddPartitionFieldExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m query_evo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mALTER TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatalog_nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_nm\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mADD PARTITION FIELD truncate(4, birthday) as year_birthday\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_evo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/iceberg_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o37.sql.\n: org.apache.iceberg.exceptions.ValidationException: Cannot bind: truncate[4] cannot transform date values from 'birthday'\n\tat org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)\n\tat org.apache.iceberg.expressions.UnboundTransform.bind(UnboundTransform.java:48)\n\tat org.apache.iceberg.expressions.UnboundTransform.bind(UnboundTransform.java:25)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.resolve(BaseUpdatePartitionSpec.java:337)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.addField(BaseUpdatePartitionSpec.java:174)\n\tat org.apache.iceberg.BaseUpdatePartitionSpec.addField(BaseUpdatePartitionSpec.java:44)\n\tat org.apache.spark.sql.execution.datasources.v2.AddPartitionFieldExec.run(AddPartitionFieldExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n\tat sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "query_evo = f'''\n",
    "ALTER TABLE {catalog_nm}.{table_nm}\n",
    "ADD PARTITION FIELD truncate(4, birthday) as year_birthday\n",
    "'''\n",
    "spark.sql(query_evo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c62816a4-c53c-4c6a-ac3e-f649c6de28e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                            |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|name                        |string                                                                                                               |null   |\n",
      "|surname                     |string                                                                                                               |null   |\n",
      "|country                     |string                                                                                                               |null   |\n",
      "|birthday                    |date                                                                                                                 |null   |\n",
      "|                            |                                                                                                                     |       |\n",
      "|# Metadata Columns          |                                                                                                                     |       |\n",
      "|_spec_id                    |int                                                                                                                  |       |\n",
      "|_partition                  |struct<birthday:date>                                                                                                |       |\n",
      "|_file                       |string                                                                                                               |       |\n",
      "|_pos                        |bigint                                                                                                               |       |\n",
      "|_deleted                    |boolean                                                                                                              |       |\n",
      "|                            |                                                                                                                     |       |\n",
      "|# Detailed Table Information|                                                                                                                     |       |\n",
      "|Name                        |iceberg_catalog.my_database.my_part_table                                                                            |       |\n",
      "|Type                        |MANAGED                                                                                                              |       |\n",
      "|Location                    |warehouse_loc/my_database/my_part_table                                                                              |       |\n",
      "|Provider                    |iceberg                                                                                                              |       |\n",
      "|Owner                       |victorgalan                                                                                                          |       |\n",
      "|Table Properties            |[current-snapshot-id=401056988078697897,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]|       |\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {my_table}\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8453388d-ea55-4d9d-b26a-8e13f5372bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------------+----------+\n",
      "|           name|surname|             country|  birthday|\n",
      "+---------------+-------+--------------------+----------+\n",
      "| Anthony Henson| Campos|              Zambia|2016-08-26|\n",
      "|   Daniel James|  Davis|Cocos (Keeling) I...|2017-03-10|\n",
      "|   Jason Graves|Raymond|              Jordan|2017-05-28|\n",
      "|   Molly Knight|Edwards|                Togo|2018-09-20|\n",
      "|Amanda Garrison|  White|       Liechtenstein|2022-05-28|\n",
      "+---------------+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT *\n",
    "FROM {my_table}\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78462cd6-5da3-429a-9509-8a0c021a9db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "INSERT INTO {my_table}\n",
    "SELECT name, \n",
    "        surname, \n",
    "        country, \n",
    "        to_date(birthday, 'yyyy-MM-dd') AS birthday\n",
    "FROM fake_data;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8ce945d-49f7-4bed-8bbf-5ba72296179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------+-------+\n",
      "|col_name                    |data_type                                |comment|\n",
      "+----------------------------+-----------------------------------------+-------+\n",
      "|name                        |string                                   |null   |\n",
      "|surname                     |string                                   |null   |\n",
      "|country                     |string                                   |null   |\n",
      "|birthday                    |date                                     |null   |\n",
      "|# Partition Information     |                                         |       |\n",
      "|# col_name                  |data_type                                |comment|\n",
      "|country                     |string                                   |null   |\n",
      "|                            |                                         |       |\n",
      "|# Metadata Columns          |                                         |       |\n",
      "|_spec_id                    |int                                      |       |\n",
      "|_partition                  |struct<birthday:date,country:string>     |       |\n",
      "|_file                       |string                                   |       |\n",
      "|_pos                        |bigint                                   |       |\n",
      "|_deleted                    |boolean                                  |       |\n",
      "|                            |                                         |       |\n",
      "|# Detailed Table Information|                                         |       |\n",
      "|Name                        |iceberg_catalog.my_database.my_part_table|       |\n",
      "|Type                        |MANAGED                                  |       |\n",
      "|Location                    |warehouse_loc/my_database/my_part_table  |       |\n",
      "|Provider                    |iceberg                                  |       |\n",
      "+----------------------------+-----------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {my_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95c40e",
   "metadata": {},
   "source": [
    "\n",
    "query_evo = f\"\"\"\n",
    "ALTER TABLE {catalog_nm}.{table_nm}\n",
    "SET TBLPROPERTIES (\n",
    "  'format-version' = '2'\n",
    ")\"\"\"\n",
    "spark.sql(query_evo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279f110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd5402c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inserts = 1\n",
    "date_range = (-40, -20)  # Años desde el presente\n",
    "\n",
    "# Lista de países (puedes usar faker.country() si prefieres)\n",
    "countries = ['United States', 'Canada', 'United Kingdom', ...]\n",
    "\n",
    "# Generar datos aleatorios\n",
    "fake = Faker()\n",
    "data = []\n",
    "for _ in range(num_inserts):\n",
    "    data.append({\n",
    "        'name': fake.name(),\n",
    "        'surname': fake.last_name(),\n",
    "        'birthday': datetime.strptime(fake.date_between(start_date=f'{date_range[0]}y', end_date=f'{date_range[1]}y').strftime('%Y-%m-%d'), '%Y-%m-%d'),\n",
    "        'country': str(random.choice(countries))  # O str(fake.random_element(elements=countries))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbf86178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joseph Ellis',\n",
       "  'surname': 'Garcia',\n",
       "  'birthday': datetime.datetime(1989, 8, 16, 0, 0),\n",
       "  'country': 'Ellipsis'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5140820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- birthday: date (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      "\n",
      "+----------+--------+------------+-------+\n",
      "|  birthday| country|        name|surname|\n",
      "+----------+--------+------------+-------+\n",
      "|1989-08-16|Ellipsis|Joseph Ellis| Garcia|\n",
      "+----------+--------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame de PySpark\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Create DataFrame from valid data\n",
    "df = spark.createDataFrame(data)\n",
    "df = df.withColumn(\"birthday\", df[\"birthday\"].cast(\"date\"))\n",
    "\n",
    "df.createOrReplaceTempView(\"fake_data\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0546ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------+-------+\n",
      "|  birthday| country|        name|surname|\n",
      "+----------+--------+------------+-------+\n",
      "|1989-08-16|Ellipsis|Joseph Ellis| Garcia|\n",
      "+----------+--------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_view = f\"\"\"\n",
    "SELECT * \n",
    "FROM fake_data\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query_view).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50e00b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "INSERT INTO {my_table}\n",
    "SELECT name, \n",
    "        surname, \n",
    "        country, \n",
    "        to_date(birthday, 'yyyy-MM-dd') AS birthday\n",
    "FROM fake_data;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df30f89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Insert into with dataframe api\n",
    "\n",
    "#df = spark.createDataFrame([Row(name=\"Sr. victor\",surname=\"galan\")])\n",
    "#df.writeTo(f\"{catalog_nm}.{table_nm}\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1292bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------------------+----------+\n",
      "|         name|surname|             country|  birthday|\n",
      "+-------------+-------+--------------------+----------+\n",
      "|    Joseph Ho| Castro|Falkland Islands ...|2019-09-01|\n",
      "|Benjamin Wong| Tucker|             Ireland|2020-09-03|\n",
      "|    Joseph Ho| Castro|Falkland Islands ...|2019-09-01|\n",
      "|Benjamin Wong| Tucker|             Ireland|2020-09-03|\n",
      "|    Joseph Ho| Castro|Falkland Islands ...|2019-09-01|\n",
      "|Benjamin Wong| Tucker|             Ireland|2020-09-03|\n",
      "+-------------+-------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query the table\n",
    "spark.sql(\n",
    "    f\"\"\"SELECT * \n",
    "    FROM {my_table};\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9b95f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2024-10-21 22:05:39.118|756897460753581554 |null               |false              |\n",
      "|2024-10-21 22:19:12.081|6660628102038648577|null               |true               |\n",
      "|2024-10-21 22:23:51.307|3516254222102115996|6660628102038648577|true               |\n",
      "|2024-10-21 22:26:11.313|3329146188426717797|3516254222102115996|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query the table\n",
    "spark.sql(\n",
    "    f\"\"\"SELECT * \n",
    "    FROM {my_table}.history;\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46a2d9-126c-4108-9bce-ae0cf7d8811b",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3f660481-00a8-47de-a3ca-b67da214560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+\n",
      "|            name|surname|      country|  birthday|\n",
      "+----------------+-------+-------------+----------+\n",
      "|     Heidi Evans| Miller|      Nigeria|2016-08-24|\n",
      "|   Mark Johnston| Melton|New Caledonia|2019-08-04|\n",
      "|   Shelly Guzman|   Todd|        Samoa|2019-10-16|\n",
      "|    Matthew Cook|  Green|         Cuba|2020-02-20|\n",
      "|Mr. Jeremy Riley| Howard|   Tajikistan|2020-04-29|\n",
      "|   Julie Mueller|  Smith|      Ireland|2020-09-21|\n",
      "+----------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\n",
    "    f\"\"\"SELECT * \n",
    "    FROM {my_table};\n",
    "    \"\"\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a223ebc9-91f3-4a8a-bf12-340c77b93030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+--------------------------+\n",
      "|name            |surname|country      |birthday  |array                     |\n",
      "+----------------+-------+-------------+----------+--------------------------+\n",
      "|Heidi Evans     |Miller |Nigeria      |2016-08-24|[Heidi Evans, Miller]     |\n",
      "|Mark Johnston   |Melton |New Caledonia|2019-08-04|[Mark Johnston, Melton]   |\n",
      "|Shelly Guzman   |Todd   |Samoa        |2019-10-16|[Shelly Guzman, Todd]     |\n",
      "|Matthew Cook    |Green  |Cuba         |2020-02-20|[Matthew Cook, Green]     |\n",
      "|Mr. Jeremy Riley|Howard |Tajikistan   |2020-04-29|[Mr. Jeremy Riley, Howard]|\n",
      "|Julie Mueller   |Smith  |Ireland      |2020-09-21|[Julie Mueller, Smith]    |\n",
      "+----------------+-------+-------------+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.withColumn(\"array\", F.array_distinct(F.array(F.col(\"name\"),F.col(\"surname\"))))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c0a268d7-91f0-4bd4-ba4f-f1d8717ef12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+--------------------------+--------------+\n",
      "|name            |surname|country      |birthday  |array                     |filtered_array|\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+\n",
      "|Heidi Evans     |Miller |Nigeria      |2016-08-24|[Heidi Evans, Miller]     |[Miller]      |\n",
      "|Mark Johnston   |Melton |New Caledonia|2019-08-04|[Mark Johnston, Melton]   |[Melton]      |\n",
      "|Shelly Guzman   |Todd   |Samoa        |2019-10-16|[Shelly Guzman, Todd]     |[Todd]        |\n",
      "|Matthew Cook    |Green  |Cuba         |2020-02-20|[Matthew Cook, Green]     |[Green]       |\n",
      "|Mr. Jeremy Riley|Howard |Tajikistan   |2020-04-29|[Mr. Jeremy Riley, Howard]|[Howard]      |\n",
      "|Julie Mueller   |Smith  |Ireland      |2020-09-21|[Julie Mueller, Smith]    |[Smith]       |\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the array column\n",
    "df = df.withColumn(\"array\", F.array_distinct(F.array(F.col(\"name\"), F.col(\"surname\"))))\n",
    "\n",
    "# Filter out elements from 'array' that match the value in 'name'\n",
    "df = df.withColumn(\n",
    "    \"filtered_array\",\n",
    "    F.expr(\"filter(array, x -> x != name)\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "279d8bfb-8422-4bb0-8564-6a0e415a9cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+--------------------------+--------------+----------------+\n",
      "|name            |surname|country      |birthday  |array                     |filtered_array|filtered_string |\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+----------------+\n",
      "|Heidi Evans     |Miller |Nigeria      |2016-08-24|[Heidi Evans, Miller]     |[Miller]      |Heidi Evans     |\n",
      "|Mark Johnston   |Melton |New Caledonia|2019-08-04|[Mark Johnston, Melton]   |[Melton]      |Mark Johnston   |\n",
      "|Shelly Guzman   |Todd   |Samoa        |2019-10-16|[Shelly Guzman, Todd]     |[Todd]        |Shelly Guzman   |\n",
      "|Matthew Cook    |Green  |Cuba         |2020-02-20|[Matthew Cook, Green]     |[Green]       |Matthew Cook    |\n",
      "|Mr. Jeremy Riley|Howard |Tajikistan   |2020-04-29|[Mr. Jeremy Riley, Howard]|[Howard]      |Mr. Jeremy Riley|\n",
      "|Julie Mueller   |Smith  |Ireland      |2020-09-21|[Julie Mueller, Smith]    |[Smith]       |Julie Mueller   |\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the array column\n",
    "df = df.withColumn(\"array\", F.array_distinct(F.array(F.col(\"name\"), F.col(\"surname\"))))\n",
    "\n",
    "# Filter out elements from 'array' that match the value in 'name'\n",
    "# Then join the filtered array elements into a single string, separated by a comma\n",
    "df = df.withColumn(\n",
    "    \"filtered_string\",\n",
    "    F.expr(\"array_join(filter(array, x -> x != name), ', ')\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "758c43ce-0436-4059-b965-c121f5fd703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-------------+----------+--------------------------+--------------+---------------+\n",
      "|name            |surname|country      |birthday  |array                     |filtered_array|filtered_string|\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+---------------+\n",
      "|Heidi Evans     |Miller |Nigeria      |2016-08-24|[Heidi Evans, Miller]     |[Miller]      |Miller         |\n",
      "|Mark Johnston   |Melton |New Caledonia|2019-08-04|[Mark Johnston, Melton]   |[Melton]      |Melton         |\n",
      "|Shelly Guzman   |Todd   |Samoa        |2019-10-16|[Shelly Guzman, Todd]     |[Todd]        |Todd           |\n",
      "|Matthew Cook    |Green  |Cuba         |2020-02-20|[Matthew Cook, Green]     |[Green]       |Green          |\n",
      "|Mr. Jeremy Riley|Howard |Tajikistan   |2020-04-29|[Mr. Jeremy Riley, Howard]|[Howard]      |Howard         |\n",
      "|Julie Mueller   |Smith  |Ireland      |2020-09-21|[Julie Mueller, Smith]    |[Smith]       |Smith          |\n",
      "+----------------+-------+-------------+----------+--------------------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the array column with distinct values and filter out nulls\n",
    "df = df.withColumn(\"array\", \n",
    "                   F.expr(\"filter(array_distinct(array(name, surname)), x -> x IS NOT NULL)\"))\n",
    "\n",
    "# Remove elements from 'array' that match the value in 'name'\n",
    "filtered_array = F.expr(\"filter(array, x -> x != name)\")\n",
    "\n",
    "# Add the filtered array back to the DataFrame\n",
    "df = df.withColumn(\"filtered_array\", filtered_array)\n",
    "\n",
    "# Join the filtered array into a single string\n",
    "df = df.withColumn(\"filtered_string\", F.concat_ws(\", \", F.col(\"filtered_array\")))\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f41611-7e4c-4204-9deb-72dc22bec9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (iceberg_env)",
   "language": "python",
   "name": "iceberg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
